{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#home","text":"For full documentation visit mkdocs.org .","title":"Home"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/","text":"Large-sample confidence intervals for a proportion Summary The aim of this notebook is to produce a large sample approximate 99% confidence interval for the proportion of machinists who experienced at least one accident over a period of time. The confidence interval was calculated to be (0.228, 0.342). Introduction The report is concerned with producing an interval estimate for the proportion of machinists who suffered at least one accident over a period of time. The data is historical, and constists of a single data field that contains the number of accidents suffered by each of the 414 machinists over the period of observation. The data was provided by M248: Analysing data, who in turn sourced it from Greenwood, M. and Yue, G.U. (1920). Method An estimate of the proportion of machinists who suffered an accident was calculated. A normal approximated on the binomial was then used to calculate a large sample approximate 99% confidence interval for the proportion. We can use a normal approximation given the sample size ( \\(n=\\) 414). All calculations were done in Python, using the following packages: pandas , statsmodels , seaborn (visualisations), and matplotlib (visualisations). Results Setup the notebook # import the packages from src import load from statsmodels.stats.proportion import proportion_confint import seaborn as sns import matplotlib.pyplot as plt # set the Seaborn theme sns.set_theme() # load the data accidents = load.accidents() Visualise the data sns.countplot(data=accidents, x=\"Accidents\") plt.show() Return the \\(z\\) -interval # declare the parameters x = accidents.query('Accidents != 0').index.size n = accidents.index.size # get the estimate proportion x/n 1 0.28502415458937197 # return the confidence interval proportion_confint(count=x, nobs=n, alpha=0.01) 1 (0.22787583995834104, 0.3421724692204029) References Greenwood, M. and Yue, G.U. (1920). 'An inquiry into the nature of frequency distributions representative of happening with the particular reference to the occurrence of multiple attacks of disease or of repeated accidents', Journal of the Royal Statistics Society , vol. 83, no. 2, pp. 255-79.","title":"Large-sample confidence intervals for a proportion"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/#large-sample-confidence-intervals-for-a-proportion","text":"","title":"Large-sample confidence intervals for a proportion"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/#summary","text":"The aim of this notebook is to produce a large sample approximate 99% confidence interval for the proportion of machinists who experienced at least one accident over a period of time. The confidence interval was calculated to be (0.228, 0.342).","title":"Summary"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/#introduction","text":"The report is concerned with producing an interval estimate for the proportion of machinists who suffered at least one accident over a period of time. The data is historical, and constists of a single data field that contains the number of accidents suffered by each of the 414 machinists over the period of observation. The data was provided by M248: Analysing data, who in turn sourced it from Greenwood, M. and Yue, G.U. (1920).","title":"Introduction"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/#method","text":"An estimate of the proportion of machinists who suffered an accident was calculated. A normal approximated on the binomial was then used to calculate a large sample approximate 99% confidence interval for the proportion. We can use a normal approximation given the sample size ( \\(n=\\) 414). All calculations were done in Python, using the following packages: pandas , statsmodels , seaborn (visualisations), and matplotlib (visualisations).","title":"Method"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/#results","text":"","title":"Results"},{"location":"1.%20Confidence%20Intervals/b-04-zinterval-proportion/#references","text":"Greenwood, M. and Yue, G.U. (1920). 'An inquiry into the nature of frequency distributions representative of happening with the particular reference to the occurrence of multiple attacks of disease or of repeated accidents', Journal of the Royal Statistics Society , vol. 83, no. 2, pp. 255-79.","title":"References"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/","text":"Debasement of silver coins during the late Byzantine Empire Summary Introduction The aim of this to report is to compare the mean silver content of two samples of silver coins minted by the Byzantine empire in the twelfth-century, during the reign of Manuel I Comnensus. The data contains two samples of the silver content of coins minted during different periods of his rulership. The exact periods of the minting are unknown, and the unit of measurement is not given. This does not affect the analysis, but comparisons with other similar data suggests the units are in grams. The data was taken from the Open University course M248: Analysing data. No reference to the source data was given. Method Results The distribution of the silver content of the coins is shown below. The plot shows that the distribution of the coins from the earlier period ( Coin1 ) generally had a higher silver content compared to coins from the later period ( Coin4 ). The data were approximately normally distributed, as confirmed by the two probability plots shown below. Both distributions approximately follow a straight line, which is consistent with normally distributed data. The coins from the later period have a greater degree of spread in the points, but they still follow the line close enough to assume normality. We checked the assumption that there was a common population variance between the two samples calculating if the ratio of the two sample's standard deviations was less that 3. The ratio was calculated as approximately 2.2, which is less than 3, so the assumption was justified. The mean silver content in coins from the earlier period was approximately 6.7g, with 95% \\(t\\) -interval (6.3, 7.2). Similarly, the mean silver content in coins from the later period was approximately 5.6g with 95% \\(t\\) -interval (5.3, 5.9). A two-sample, one-sided \\(t\\) -test of the hypothesis of equal means gave \\(t=\\) 4.73 on 14 degrees of freedom, with \\(p\\) -value of 0.0003. Discussion With \\(p<\\) 0.01, there is strong evidence against the null hypothesis that the mean silver content in both period's coins are equal. The data indicates that the coins from the later period had a lower mean silver content compared to coins from the earlier period.","title":"Debasement of silver coins during the late Byzantine Empire"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/#debasement-of-silver-coins-during-the-late-byzantine-empire","text":"","title":"Debasement of silver coins during the late Byzantine Empire"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/#summary","text":"","title":"Summary"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/#introduction","text":"The aim of this to report is to compare the mean silver content of two samples of silver coins minted by the Byzantine empire in the twelfth-century, during the reign of Manuel I Comnensus. The data contains two samples of the silver content of coins minted during different periods of his rulership. The exact periods of the minting are unknown, and the unit of measurement is not given. This does not affect the analysis, but comparisons with other similar data suggests the units are in grams. The data was taken from the Open University course M248: Analysing data. No reference to the source data was given.","title":"Introduction"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/#method","text":"","title":"Method"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/#results","text":"The distribution of the silver content of the coins is shown below. The plot shows that the distribution of the coins from the earlier period ( Coin1 ) generally had a higher silver content compared to coins from the later period ( Coin4 ). The data were approximately normally distributed, as confirmed by the two probability plots shown below. Both distributions approximately follow a straight line, which is consistent with normally distributed data. The coins from the later period have a greater degree of spread in the points, but they still follow the line close enough to assume normality. We checked the assumption that there was a common population variance between the two samples calculating if the ratio of the two sample's standard deviations was less that 3. The ratio was calculated as approximately 2.2, which is less than 3, so the assumption was justified. The mean silver content in coins from the earlier period was approximately 6.7g, with 95% \\(t\\) -interval (6.3, 7.2). Similarly, the mean silver content in coins from the later period was approximately 5.6g with 95% \\(t\\) -interval (5.3, 5.9). A two-sample, one-sided \\(t\\) -test of the hypothesis of equal means gave \\(t=\\) 4.73 on 14 degrees of freedom, with \\(p\\) -value of 0.0003.","title":"Results"},{"location":"2.%20Hypothesis%20Testing/01_byzantine_coins/#discussion","text":"With \\(p<\\) 0.01, there is strong evidence against the null hypothesis that the mean silver content in both period's coins are equal. The data indicates that the coins from the later period had a lower mean silver content compared to coins from the earlier period.","title":"Discussion"},{"location":"Past%20Paper%20Solutions/m248-201806/","text":"Section A q.01 A. Calculating probabilities using a probability function (HB p.7) : p.d.f. If \\(f(x) = \\frac{2}{9}(1+x)\\) , \\(x \\in (-1, 2)\\) then the probability \\(P(0 \\leq X < 1)\\) is given by \\[ \\begin{aligned} P(0 \\leq X < 1) &= \\int_{0}^{1} \\frac{2}{9}(1+x) \\> dx \\\\ &= \\frac{2}{9} \\bigg[ x + \\frac{1}{2}x^{2} \\bigg]_{0}^{1} \\\\ &= \\frac{2}{9} \\bigg(1+\\frac{1}{2} - (0+0) \\bigg) = \\frac{1}{3}. \\end{aligned} \\] q.02 C. Normalising constants (Book A, p.111) Normalising constant \\(K\\) is given by \\[ \\begin{aligned} 1 &= \\int_{0}^{1} \\frac{1}{K}x^{4} \\> dx = \\frac{1}{K} \\int_{0}^{1} x^{4} \\> dx. \\end{aligned} \\] Therefore, \\(K\\) is \\[ K = \\int_{0}^{1} x^{4} \\> dx = \\bigg[ \\frac{1}{5} x^{5} \\bigg]_{0}^{1} = \\frac{1}{5} ( 1 - 0 ) = \\frac{1}{5}. \\] q.03 F. Calculating probabilities using the c.d.f. : discrete The \\(P(X \\leq 4)\\) is given by \\[ \\begin{aligned} P(X \\leq 4) = F(4) &= p(0) + \\cdots + p(4) \\\\ &= 0.1 + \\cdots + 0.1 \\\\ &= 0.6. \\end{aligned} \\] q.04 D. Calculating probabilities using the c.d.f. : continuous The probability \\(P(1 \\leq X \\leq 2) = F(2) - F(1)\\) is given by \\[ \\begin{aligned} F(2) - F(1) &= 1 - \\frac{1}{3} \\sqrt{9 - 2^{2}} - \\bigg( 1 - \\frac{1}{3} \\sqrt{9 - 1^{2}} \\bigg) \\\\ &= 1 - \\frac{1}{3} \\sqrt{5} - 1 + \\frac{1}{3} \\sqrt{8} \\\\ &= \\frac{1}{3} (\\sqrt{8} - \\sqrt{5}). \\end{aligned} \\] q.05 E. Choosing a model based on the range of the standard probability models (HB p.26, 27 ) The range of \\(X\\) is \\(\\{1, 2, 3, 4, 5\\}\\) . Cannot be a Bernoulli , as range of Bernoulli is \\(\\{0,1\\}\\) . Cannot be a binomial , as range of Binomial includes \\(0\\) . Cannot be a geometric or Poisson , as these have no upper boundaries ( \\(X\\) has max value of 5). Cannot be a continuous, as \\(X\\) is discrete. Therefore it must be a discrete uniform distribution. q.06 E. Mean of a rv (HB p.7) : discrete The \\(E(X) = \\sum_{x} x \\>p(x)\\) , so \\[ \\begin{aligned} \\sum_{x} x \\>p(x) = 0(0.1) + 1(0.25) + \\cdots + 4(0.2) = 2.15. \\end{aligned} \\] q.07 E. Mean of a linear function of a rv (HB p.9) For a random variable \\(Y = 20 - 3X\\) , then \\[ E(Y) = aE(X) + b, \\] where \\(E(X) = 1\\) , \\(a = -3\\) , and \\(b=20\\) . Therefore, \\[ E(Y) = -3(1) + 20 = 17. \\] q.08 B. Variance of a linear function of rvs (HB p.9) For a random variable \\(Y = 20 - 3X\\) , then \\[ S(Y) = \\sqrt{V(Y)} = \\sqrt{a^{2} V(X)} = |a| \\sqrt{V(X)}, \\] where \\(V(X) = 4\\) and \\(a = -3\\) . Therefore, \\[ S(Y) = |-3| \\sqrt{4} = 6. \\] q.09 B. Poisson distribution (HB p.8) and Poisson process (HB p.10) Let \\(X\\) model the number of email arriving in the office per hour. Then \\(X \\sim \\text{Poisson}(\\mu)\\) , where \\(\\mu = 5\\) is the average number of emails received in an hour. The probability \\(P(X=3)\\) is given by \\[ \\begin{aligned} p(3) = e^{-5} \\bigg( \\frac{5^{3}}{3!} \\bigg) \\simeq 0.140. \\end{aligned} \\] q.10 D. Poisson distribution (HB p.8) and Poisson process (HB p.10) The \\(P(X < 3) = P(X \\leq 2)\\) is given by \\[ \\begin{aligned} P(X \\leq 2) = \\sum_{i=0}^{2} e^{-5} \\bigg( \\frac{5^{i}}{i!} \\bigg) &= e^{-5} \\sum_{i=0}^{2} \\frac{5^{i}}{i!} \\\\ &= e^{-5} \\bigg( \\frac{5^{0}}{0!} + \\frac{5^{1}}{1!} + \\frac{5^{2}}{2!} \\bigg) \\\\ &= e^{-5} (1 + 5 + 12.5) \\\\ &\\simeq 0.125. \\end{aligned} \\] q.11 D. Exponential distribution (HB p.10) and Poisson process (HB p.10) Let \\(T\\) model the waiting time between emails arriving in the office. Then \\(T \\sim M(\\lambda)\\) , where \\(\\lambda = 5\\) is the average number of emails received in an hour. Ten minutes is \\(1/6\\) hours, so \\(P(T < 1/6)\\) is given by \\[ \\begin{aligned} P \\bigg(T \\leq \\frac{1}{6} \\bigg) = F\\bigg(\\frac{1}{6}\\bigg) = 1 - e^{-5 \\big( \\frac{1}{6} \\big)} \\simeq 0.565. \\end{aligned} \\] q.12 F. Transforming the parameter of a Poisson distribution (HB p.10) The number of emails that will arrive in 3 hours is distributed \\(\\text{Poisson}(\\lambda t)\\) , where \\(\\lambda = 5\\) emails per hour and \\(t=3\\) . Therefore \\(\\text{Poisson}(15)\\) . q.13 A. Population quantiles of a rv (HB p.11) : continuous The \\(\\alpha\\) -quantile of a continuous rv \\(X\\) with c.d.f. \\(F(X)\\) is defined as \\(F(x) = \\alpha\\) . If \\(F(x) = 1 - \\sqrt{1-x}\\) and \\(\\alpha = 1/4\\) , then it must be that \\[ \\begin{aligned} F(x) = \\alpha \\to 1 - \\sqrt{1-x} &= \\frac{1}{4} \\\\ 1 - \\frac{1}{4} &= \\sqrt{1-x} \\\\ \\bigg(\\frac{3}{4} \\bigg)^{2} &= 1 - x \\\\ x &= 1 - \\frac{9}{16} \\\\ &= \\frac{7}{16}. \\end{aligned} \\] q.14 C. Difference between two independent normal rvs (HB p.11) If \\(X \\sim N(2,4)\\) , \\(Y \\sim N(1,3)\\) , then \\(U = X - Y\\) will also be normally distributed with parameters, \\[ E(U) = E(X-Y) = E(X) - E(Y) = 2 - 1 = 1, \\] and \\[ V(U) = V(X-Y) = V(X) + V(Y) = 4 + 3 = 7. \\] Hence \\(U = X - Y \\sim N(1, 7)\\) . q.15 C. Probabilities for a normal distribution (HB p.12) We know that \\(X \\sim N(100, 15^{2})\\) . Then the probability \\(P(90 < X < 125)\\) is given by \\[ \\begin{aligned} P(X < 125) - P(X < 90) &= P\\bigg(Z < \\frac{125-100}{15} \\bigg) - P\\bigg(Z < \\frac{90-100}{15} \\bigg) \\\\ &\\simeq \\Phi(1.67) - \\Phi(-0.67) \\\\ &= \\Phi(1.67) - (1 - \\Phi(0.67)) \\\\ &= 0.9525 + 0.7486 - 1 \\simeq 0.701. \\end{aligned} \\] q.16 B. Distribution of the sample mean (HB p.12) We know \\(\\mu_{X} = 69.1\\) , \\(\\sigma^{2}_{X} = 9.4\\) and \\(n=40\\) . Then \\(\\overline{X}_{40}\\) will have the distribution \\(\\overline{X}_{40} \\sim N(\\mu_{X}, \\sigma^{2}_{X}/n) = N (69.1, 0.235)\\) . And so \\(P(X < 68)\\) is given by \\[ \\begin{aligned} P(X < 68) &= P\\bigg(Z < \\frac{68-69.1}{\\sqrt{0.235}} \\bigg) \\\\ &\\simeq \\Phi(-2.27) \\\\ &= 1 - \\Phi(2.27) \\\\ &= 1 - 0.9884 \\simeq 0.012. \\end{aligned} \\] q.17 C. Distribution of the sample total (HB p.13) and quantiles of any non-standard normal (HB p.12) The sample total \\(T_{n}\\) is distributed \\(T_{n} \\sim N \\big( n \\mu, (\\sqrt{n} \\sigma)^{2} \\big)\\) . If \\(X \\sim N(\\mu, \\sigma^{2})\\) , then the \\(\\alpha\\) -quantile \\(x\\) is given by \\(x = \\sigma q_{\\alpha} + \\mu\\) . Therefore the \\(\\alpha\\) -quantile of \\(s\\) will be given by \\[ s = q_{\\alpha} \\sqrt{n \\sigma^{2}} + n \\mu = q_{\\alpha} \\sigma \\sqrt{n} + n \\mu. \\] q.18 C. Variance of an unbiased estimator (U7.1, Ex.6) If \\(X_{1} \\sim N(\\mu, 1)\\) , \\(X_{2} \\sim N(\\mu, 4)\\) , \\(X_{3} \\sim N(\\mu, 4)\\) and \\(\\widehat{\\mu} = \\frac{1}{6} (4X_{1} + X_{2} + X_{3})\\) , then \\[ \\begin{aligned} V(\\widehat{\\mu}) = V\\bigg( \\frac{1}{6} (4X_{1} + X_{2} + X_{3}) \\bigg) &= \\bigg( \\frac{1}{6} \\bigg)^{2} V(4X_{1} + X_{2} + X_{3}) \\\\ &= \\frac{1}{36} \\bigg\\{ V(4X_{1}) + V(X_{2}) + V(X_{3}) \\bigg\\} \\\\ &= \\frac{1}{36} \\{ 4^{2} \\> V(X_{1}) + V(X_{2}) + V(X_{3}) \\} \\\\ &= \\frac{1}{36} \\{ 16 \\> (1) + 4 + 4 \\} \\\\ &= \\frac{1}{36} \\{ 24 \\} \\\\ &= \\frac{2}{3}. \\end{aligned} \\] q.19 A. Transforming a confidence interval (HB p.14) The transformation \\(X = \\frac{5}{9} (Y-32)\\) is both linear and increasing. When \\(Y=245.3\\) , \\[X = \\frac{5}{9} (245.3-32) = 118.5,\\] and \\(Y=249.8\\) , \\[X = \\frac{5}{9} (249.8-32) = 121.0.\\] Therefore, a new 95% confidence interval in degrees Celsius is \\((118.5, 121.0)\\) . q.20 E. Critical values (HB p.16) of a t-test (HB p.17) The critical value of a one-tail test \\(t\\) -test is the \\((1-\\alpha)\\) -quantile of \\(t(\\nu)\\) , where \\(\\nu\\) is the degrees of freedom. The test is at a 1% significance level so \\(\\alpha=0.01\\) , and has a sample size \\(n=101\\) , meaning \\(q_{1-\\alpha} = 0.99\\) and \\(\\nu=n-1=101-1=100\\) . Therefore the critical value is the 0.99 -quantile of \\(t(100)\\) , which is 2.364. q.21 Significance level of a hypothesis test (HB p.17) and Interpreting a \\(p\\) -values (HB p.18) B. False . \\(p\\) -value is the significance probability , not the probability \\(H_{0}\\) is correct. E. False . The significance level is \\(100\\alpha = 1\\%\\) , so \\(\\alpha = 0.01\\) . We can see that \\(p = 0.02 > 0.01 = \\alpha\\) , so \\(H_{0}\\) is not rejected. q.22 A. Calculating the power of a hypothesis test (HB p.18) The power of this one-sided test at a 5% significance level will be \\[ 1 - \\Phi \\bigg( q_{1-\\alpha} - \\frac{d}{\\sigma/\\sqrt{n}} \\bigg) \\] where \\(q_{1-\\alpha} = q_{0.95} = 1.645\\) , \\(d=0.2\\) , \\(\\sigma=0.25\\) , and \\(n=20\\) . Therefore, \\[ \\begin{aligned} 1 - \\Phi \\bigg( q_{1-\\alpha} - \\frac{d}{\\sigma/\\sqrt{n}} \\bigg) &= 1 - \\Phi \\bigg( 1.645 - \\frac{0.2}{0.25/\\sqrt{20}} \\bigg) \\\\ &\\simeq 1 - \\Phi (-1.93) \\\\ &= 1 - \\{1 - \\Phi(1.93)\\} \\\\ &= \\Phi (1.93) \\\\ &= 0.9732. \\end{aligned} \\] q.23 A. Choosing the sample size of a hypothesis test (HB p.19) The required sample size \\(n\\) of a two-sided hypothesis test at a 5% significance level is given by \\[ n = \\frac{\\sigma}{d^{2}} (q_{1-\\alpha/2} - q_{1-\\gamma})^{2}, \\] where \\(q_{1-\\alpha /2} = q_{0.975} = 1.960\\) , \\(d=0.25\\) , \\(\\sigma=0.1\\) , and \\(q_{1- \\gamma} = q_{0.15} = -1.036\\) . Therefore, \\[ \\begin{aligned} n &= \\frac{1}{0.25^{2}} (1.960 - - 1.036)^{2} \\\\ &= 16(1.960 + 1.036)^{2} \\\\ &= 143.6 \\simeq 144. \\end{aligned} \\] q.24 C. Degrees of freedom of a Chi-squared goodness-of-fit test (HB p.21) The degrees of freedom of a chi-squared goodness-of-fit test is \\(k-p-1\\) , where \\(k=4\\) is the number of categories, and \\(p=1\\) is the number of estimated parameters. Therefore the degrees of freedom is \\(\\nu=4-1-1=2\\) . q.25 F. Calculating the test statistic of Chi-squared goodness-of-fit test (HB p.21) The chi-squared test statistic is \\[ \\chi^{2} = \\sum\\frac{(O_{i} - E{i})^{2}}{E_i}, \\] so in this case, the chi-squared test statistic will be \\[ \\chi^{2} = \\frac{(44-55.4)^{2}}{55.4} + \\dots + \\frac{(10-6.2)^{2}}{6.2} \\simeq 6.94. \\] q.26 A. Calculating \\(S_{xx}\\) , \\(S_{yy}\\) and \\(S_{xy}\\) (HB p.22) The value of \\(S_{yy}\\) is given by \\[ S_{yy} = \\sum y_{i}^{2} - \\frac{\\{\\sum y_{i}\\} ^{2}}{n}. \\] We know that \\(n=14\\) , \\(\\sum y_{i}^{2} = 34692\\) , and \\(\\sum y_{i} = 600\\) . Therefore, \\[ S_{yy} = 34692 - \\frac{600^{2}}{14} = 8977.7142 \\ldots \\simeq 8977.7. \\] q.27 C. Calculating \\(S_{xx}\\) , \\(S_{yy}\\) , and \\(S_{xy}\\) (HB p.22) Value of \\(S_{xy}\\) is given by \\[ S_{xy} = \\sum x_{i} \\> y_{i} - \\frac{\\sum x_{i} \\sum y_{i}}{n}. \\] We know that \\(n=14\\) , \\(\\sum x_{i} \\> y_{i} = 6912\\) , \\(\\sum x_{i} = 118\\) , and \\(\\sum y_{i} = 600\\) . Therefore, \\[ S_{xy} = 6912 - \\frac{(118)(600)}{14} = 1854.8571 \\ldots \\simeq 1854.9. \\] q.28 B . Confidence intervals for \\(\\widehat{\\beta}\\) (HB p.22) A \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\widehat{\\beta}\\) is given by \\[ \\widehat{\\beta} \\pm t \\frac{s}{\\sqrt{S_{xx}}}. \\] We know that \\(\\widehat{\\beta} = 0.76 \\> s^{2} = 91.5, \\> S_{xx} = 574.29\\) , and \\(t\\simeq 2.086\\) is the \\(0.975\\) -quantile of \\(t(n-2) = t(19) = 2.093\\) . Therefore, \\[ (\\beta^{-}, \\beta^{+}) = \\bigg( 0.76 \\pm 2.083 \\sqrt{\\frac{91.5}{574.24}} \\bigg) \\simeq (-0.072), \\] and so a 95% confidence interval for \\(\\widehat{\\beta} \\simeq (-0.08, 1.60)\\) . q.29 F . Elements of a statistical report (HB p.24) The Discussion should contain your assessment of the statistical evidence relating to the original question or hypothesis. q.30 A . Elements of a statistical report (HB p.24) The Method should include [...] the statistical test used to check the model... Section B q.31 (a) Binomial distribution (HB p.8) Let \\(X\\) be a discrete rv that represents the number of apples that passes Ping's acceptability criteria. Then \\(X\\) is modelled by the binomial distribution, \\(X \\sim B(n,p)\\) , where \\(n=5\\) is the sample size and \\(p=0.4\\) is the probability that an individual apple passes the acceptability criteria, so \\(X \\sim B(5, 0.4)\\) . The probability \\(P(X=2)\\) is given by \\[ \\begin{aligned} P(X=2) = \\binom{n}{x} p^x (1-p)^{n-x} = \\binom{5}{2} 0.4^{2} (1-0.4)^{3} = 0.3456. \\end{aligned} \\] (b) Geometric distribution (HB p.8) Let \\(Y\\) be a discrete rv that represents the sample number of the apple that passes Ping's acceptability criteria in a sample. Then \\(Y\\) is modelled by the geometric distribution, \\(Y \\sim G(p)\\) , where \\(p=0.4\\) is the probability that an individual apple passes the acceptability criteria, so \\(Y \\sim G(0.4)\\) . The probability \\(P(Y=3)\\) is given by \\[ \\begin{aligned} P(Y=3) = (1-p)^{y-1} p = 0.6^{2} 0.4 = 0.144. \\end{aligned} \\] (c) Expected value of a standard probability model (HB p.26, 27) The expected number of apples, \\(E(Y)\\) , Ping is needed to examine before finding one that meets her acceptability criteria is given by \\[ E(Y) = \\frac{1}{p} = \\frac{1}{0.4} = 2.5. \\] q.32 Variance of a rv (HB p.9) : continuous The variance \\(V(X)\\) of a continuous random variable \\(X\\) is defined as \\[ V(X) = E\\{ (X-\\mu)^{2} \\} = E(X^{2}) - E(X)^{2} \\] We know that \\(E(X) = 3/5\\) , so let us calculate \\(E(X^{2})\\) , \\[ \\begin{aligned} E(X^{2}) &= \\int_{0}^{1} x^{2} \\> \\{12 x^{2} (1-x) \\} \\> dx \\\\ &= 12 \\int_{0}^{1} x^{4} - x^{5} \\> dx \\\\ &= 12 \\bigg[ \\frac{1}{5} x^{5} - \\frac{1}{6} x^{6} \\bigg]_{0}^{1} \\\\ &= 12 \\bigg( \\frac{1}{5} (1)^{5} - \\frac{1}{6} (1)^{6} - (0-0) \\bigg) \\\\ &= \\frac{2}{5}. \\\\ \\end{aligned} \\] And so, \\[ \\begin{aligned} V(X) = E(X^{2}) - E(X)^{2} = \\frac{2}{5} - \\bigg( \\frac{3}{5} \\bigg)^{2} = \\frac{1}{25}. \\end{aligned} \\] Therefore, \\(V(X) = 1/25\\) . q.33 (a) Finding the likelihood function of a sample (HB p.13) If \\(f(x; \\theta) = \\theta e^{-\\theta x}\\) , then the likelihood of \\(\\theta\\) , \\(L(\\theta)\\) , will be \\[ \\begin{aligned} L(\\theta) = \\prod_{i=1}^{n} f(x_{i}; \\theta) &= \\theta e^{-\\theta x_{1}} \\times \\theta e^{-\\theta x_{2}} \\times \\theta e^{-\\theta x_{3}} \\times \\theta e^{-\\theta x_{4}} \\\\ &= \\theta e^{-\\theta (4.5)} \\times \\theta e^{-\\theta (1.5)} \\times \\theta e^{-\\theta (6)} \\times \\theta e^{-\\theta (4.4)} \\\\ &= \\theta \\times \\theta \\times \\theta \\times \\theta \\times e^{-\\theta (4.5)} \\times e^{-\\theta (1.5)} \\times e^{-\\theta (6)} \\times e^{-\\theta (4.4)} \\\\ &= \\theta^{4} \\> e^{-16.4 \\theta}. \\end{aligned} \\] Hence, we can see that \\(L(\\theta) = \\theta^{4} \\> e^{-16.4 \\theta}\\) . (b) Finding the MLE of an estimator (HB p.13) If \\(L(\\theta) = \\theta^{4} \\> e^{-16.4 \\theta}\\) then, by the product rule , \\(L'(\\theta)\\) will be given by \\[ \\begin{aligned} L'(\\theta) &= \\theta^{4}(-16.4 \\> e^{-16.4 \\theta}) + 4 \\> \\theta^{3} \\> e^{-16.4 \\theta} \\\\ &= 4 \\> \\theta^{3} \\> e^{-16.4 \\theta} - 16.4 \\> \\theta^{4} \\> e^{-16.4 \\theta} \\\\ &= \\theta^{3} e^{-16.4 \\theta} \\{4 - 16.4 \\> \\theta \\}. \\end{aligned} \\] Comparing this with the form of the equation given in the question, \\(L'(\\theta) = \\theta^{a} e^{-b \\theta} L_{1} (\\theta)\\) , we can see that \\(a=3\\) \\(b=16.4\\) \\(L_{1} (\\theta) = 4 - 16.4 \\> \\theta\\) (c) Finding the MLE of an estimator (HB p.13) The MLE of \\(\\theta\\) , \\(\\widehat \\theta\\) , is the solution to \\[ L'(\\theta) = \\theta^{3} e^{-16.4 \\theta} \\{4 - 16.4 \\> \\theta \\} = 0. \\] Given the range of \\(\\theta > 0\\) , then \\(\\theta^{3}, e^{-16.4 \\theta} > 0\\) , so this reduces to solving \\[ \\begin{aligned} 0 &= 4 - 16.4 \\> \\theta \\\\ 16.4 \\> \\theta &= 4 \\\\ \\theta &= \\frac{4}{16.4} \\simeq 0.244. \\end{aligned} \\] (d) MLE of a standard probability distribution (HB p.26m 27) The MLE of an exponential distribution is \\(\\widehat \\lambda = 1/\\overline{X}\\) . We can see from the data that \\(\\overline{x} = \\frac{1}{4} (4.5 + \\cdots + 4.4) = 16.4/4\\) . And so, \\(1/\\overline{x} = \\frac{1}{16.4/4} = \\frac{4}{16.4} \\simeq 0.244.\\) This value matches that given in q.33(c). q.34 (a) Assumptions for two-sample \\(t\\) -intervals (HB p.16) The assumption of an equal variance is valid if the larger of the sample variances divided by the smaller is less than 3. We have been told \\(s_{C}^{2} = 103.84\\) and \\(s_{N}^{2} = 115.99\\) , so \\[ \\frac{s_{N}^{2}}{s_{C}^{2}} = \\frac{115.99}{103.84} \\simeq 1.117 < 3. \\] Therefore, the assumption of equal variances is valid. (b) Exact confidence intervals for the difference between two normal means (HB p.16) Given two independent samples from distributions with a common variance, the pooled estimate of the common variance is given by \\[ s_{P}^{2} = \\frac{(n_{1}-1) s_{1}^{2} + (n_{2}-1) s_{2}^{2}}{n_{1} + n_{2} - 2}. \\] We have been given sample variances \\(s_{C}^{2} = 103.84\\) and \\(s_{N}^{2} = 115.99\\) , and sample sizes \\(n_{C} = 7\\) and \\(n_{N} = 13\\) , so \\[ \\begin{aligned} s_{P}^{2} &= \\frac{(7-1) 103.84 + (13-1) 115.99}{7 + 13 - 2} \\\\ &= \\frac{(6) 103.84 + (12) 115.99}{18} \\\\ &= \\frac{2014.92}{18} \\\\ &= 111.94. \\end{aligned} \\] Therefore the pooled estimate of the common population standard deviation is \\(\\sqrt{111.94} \\simeq 10.58\\) years. (c) Exact confidence intervals for the difference between two normal means (HB p.16) To calculate a \\(90\\% = 100(1-\\alpha)\\%\\) exact confidence interval, we require the \\((1 - (\\alpha/2))\\) -quantile of \\(t(\\nu)\\) , where \\(\\nu\\) is the degrees of freedom. Let us calculate \\(\\alpha\\) , \\[ \\begin{aligned} 90 &= 100(1-\\alpha) \\\\ \\frac{90}{100} &= 1 - \\alpha \\\\ \\alpha &= 1 - 0.9 = 0.1. \\end{aligned} \\] There will be \\(n_{C} + n_{P} - 2 = 7 + 13 - 2 = 18\\) degrees of freedom for the \\(t\\) -distribution. Therefore, we require the \\(0.95\\) -quantile of \\(t(18)\\) , which is \\(q_{0.95} = 1.734\\) . (d) Exact confidence intervals for the difference between two normal means (HB p.16) An exact confidence interval for the difference between two normal means, \\(d = \\mu_{1} - \\mu_{2}\\) , is given by \\[ \\begin{aligned} (d^{-}, d^{+}) &= d - t \\> s_{P} \\> \\sqrt{\\frac{1}{n_{C}} + \\frac{1}{n_{N}}} = 7.21 - \\bigg\\{ 1.734 (111.94)^{\\frac{1}{2}} \\bigg( \\frac{1}{7} + \\frac{1}{13} \\bigg)^{\\frac{1}{2}} \\bigg\\} \\simeq -1.39, \\\\ \\end{aligned} \\] Hence, a 90% two-sample t-interval for the difference between the population mean age of patients of the type in the study who had had a coronary event and those who have not is approximately \\((-1.39, 15.81)\\) . It can be seen the realised 90% confidence interval for \\(d\\) contains \\(0\\) , so it does not support a claim that the population ages differ. (e) Repeated experiments interpretation of confidence intervals (HB p.14) If this entire trial was repeated independently a large number of times, and on each occasion a 90% confidence interval for the difference between means were calculated, then about 90% of these intervals would contain the difference between population mean ages of patients in Group C and N, respectively. The 90% confidence interval actually observed, (-1.39, 15.81), is just one observation on a random interval, and may or may not contain the population mean. q.35 It is a one-sided test as \\(H_{1}: p > 0.5\\) . A \\(p\\) -value of 0.055 corresponds to \"weak or little evidence against the null hypothesis\", not \"little to no evidence against the null hypothesis\". A hypothesis test does not prove the null hypothesis to be true or not; it is instead a test as to whether to reject or not reject \\(H_{0}\\) . q.36 (a) The Mann\u2013Whitney test (HB p.20) Let the hypotheses be \\[ H_{0}: \\ell = 0, \\> H_{1}: \\ell \\neq 0 \\] where \\(\\ell\\) is the underlying difference in location between the populations from which the samples were drawn. (b) The Mann\u2013Whitney test (HB p.20) The test statistic \\(U_{A}\\) is the sum of the ranks in sample A, which has been defined as the horses of heavy weights. Therefore, \\[ \\begin{aligned} u_{A} = 2 + 4 + \\cdots + 19 = 96. \\end{aligned} \\] Therefore the test statistic is \\(u_{A} = 96\\) . (c) Normal approximation to the null distribution of the Mann\u2013Whitney test statistic (HB p.20) The mean and variance of the null distribution of the test statistic, \\(E(U_{A})\\) and \\(V(U_{A})\\) respectively, are given by \\[ \\begin{aligned} E(U_{A}) = \\frac{1}{2} (n_{A}) (n_{A} + n_{B} + 1) &= \\frac{1}{2} (8) (8+11+1) \\\\ &= \\frac{1}{2} (160) \\\\ &= 80, \\\\ \\end{aligned} \\] and \\[ \\begin{aligned} V(U_{A}) = \\frac{1}{12} (n_{A}) (n_{B}) (n_{A} + n_{B} + 1) &= \\frac{1}{12} (8) (11) (8+11+1) \\\\ &= \\frac{1}{12} (1760) \\\\ &= 146.666 \\ldots \\\\ &\\simeq 146.67. \\end{aligned} \\] (d) Normal approximation to the null distribution of the Mann\u2013Whitney test statistic (HB p.20) The \\(z\\) -value corresponding to the approximate standard normal null distribution for this test is given by \\[ \\begin{aligned} Z = \\frac{U_{A} - E(U_{A})}{\\sqrt{V(U_{A})}} &\\simeq \\frac{96 - 80}{\\sqrt{146.67)}} \\\\ &= \\frac{96 - 80}{\\sqrt{146.67)}} \\\\ &= 1.32114 \\ldots \\\\ &\\simeq 1.32. \\end{aligned} \\] Therefore \\(Z \\simeq 1.32\\) . (e) Calculating \\(p\\) -values (HB p.18) The test is two-sided, and so, using the table of probabilities of the standard normal distribution in the Handbook, the \\(p\\) -value is therefore \\[ \\begin{aligned} p &= 2 P(\\>|Z| \\geq 1.32\\>) \\\\ &= 2(1 - \\Phi(1.32) ) \\\\ &= 2(1 - 0.9066 ) \\\\ &= 0.1868. \\end{aligned} \\] Given that \\(p=0.1868\\) , there is little to no evidence against the null hypothesis that the location of the differences between light and heavy weighted horses is zero. q.37 (a) Interpreting the \\(p\\) -values in multiple linear regression For the two-sided test of the null hypothesis \\(H_{0} : \\beta_{1} = 0\\) , since \\(p < 0.001 < 0.01\\) , there is strong evidence to suggest that \\(\\beta_{1}\\) is not equal to zero. Likewise, for the two-sided test of the null hypothesis \\(H_{0} : \\beta_{2} = 0\\) , since \\(p < 0.001 < 0.01\\) , there is strong evidence to suggest that \\(\\beta_{2}\\) is not equal to zero. Therefore, there is strong evidence that both explanatory variables ( \\(x_{1}\\) and \\(x_{2}\\) ) together influence the annual profit of a small campany. (b) There is no particular pattern in the residual plot, so it seems the assumption that the residuals come from a distribution with zero mean and constant variance is a reasonable one. The points in the normal probability plot roughly follows a straight line, so the assumption that the residuals are normally distributed is also a reasonable one. (c) Using a fitted model (HB p.22) to find the residual value Using the fitted model, a small company with \\(x_{1} = 8\\) and \\(x_{2} = 64\\) is predicted to have annual profits (in \u00a3100,000s) \\[y = 0.930 - 0.3662 (8) + 0.03543 (64) \\simeq 0.268.\\] Given the actual value was \\(y=0.292\\) , this gives a residual value of \\[ w = 0.292 - 0.268 = 0.024. \\] (d) The new fitted model \\(y = \\alpha + \\beta e^{\\lambda x}\\) is not linear. Hence, the new model could not be used to fit the data using the method of least squares.","title":"M248 June 2018"},{"location":"Past%20Paper%20Solutions/m248-201806/#section-a","text":"","title":"Section A"},{"location":"Past%20Paper%20Solutions/m248-201806/#section-b","text":"","title":"Section B"},{"location":"Past%20Paper%20Solutions/m248-201906/","text":"Section A q.01 E. Checking the validity of a probability function (HB p.7) : discrete A. No, \\(\\sum x \\neq 1\\) B. No, \\(p(2) \\leq 0\\) C. No, \\(p(2) \\leq 0\\) D. No, \\(\\sum x \\neq 1\\) E. Yes, satisfies both properties. F. No, \\(\\sum x \\neq 1\\) q.02 C. Calculating probabilities using a probability function (HB p.7) : p.d.f. The probability \\(P(2 \\leq X \\leq 4)\\) is given by \\[ \\begin{aligned} P(2 \\leq X \\leq 4) = \\int_{2}^{4} \\frac{5}{4x^{2}} \\> dx &= \\frac{5}{4} \\int_{2}^{4} x^{-2} \\> dx \\\\ &= \\frac{5}{4} \\bigg[-x^{-1} \\bigg]_{2}^{4} \\\\ &= -\\frac{5}{4} \\bigg(\\frac{1}{4} - \\frac{1}{2} \\bigg) \\\\ &= \\frac{5}{16} \\\\ \\end{aligned} \\] q.03 A. Normalising constants (Book A, p.111) Normalising constant \\(K\\) is given by \\[ 1 = \\int_{-1}^{1} \\frac{1}{K} \\bigg( 1 + \\frac{x}{2} \\bigg) \\> dx = \\frac{1}{K} \\int_{-1}^{1} 1 + \\frac{x}{2} \\> dx. \\] Therefore, \\[ K = \\int_{-1}^{1} 1 + \\frac{x}{2} \\> dx = \\bigg[ x + \\frac{1}{2} \\bigg(\\frac{x^{2}}{2} \\bigg) \\bigg]_{-1}^{1} = \\bigg\\{ 1 + \\frac{1}{4} (1^{2}) - \\bigg( -1 + \\frac{1}{4} (-1)^{2} \\bigg) \\bigg\\} = 2 \\] q.04 E. Calculating probabilities using the c.d.f. : discrete The c.d.f. of \\(X\\) at \\(x=3\\) is given by \\[ F(x=3) = P(X \\leq 3) = p(1) + p(2) + p(3) = 0.1 + 0.3 + 0.2 = 0.6. \\] q.05 E. Poisson distribution (HB p.8) The \\(P(X>2) = 1 - P(X \\leq 2)\\) is given by, \\[ \\begin{aligned} 1 - P(X \\leq 2) &= 1 - p(0) + p(1) + p(2) \\\\ &= 1 - \\sum_{i=0}^{2} e^{-\\mu} \\bigg( \\frac{\\mu^{x_{i}}}{x_{i}!} \\bigg) \\\\ &= 1 - e^{-\\mu} \\sum_{i=0}^{2} \\bigg( \\frac{\\mu^{x_{i}}}{x_{i}!} \\bigg) \\\\ &= 1 - e^{-0.4} \\bigg( \\frac{0.4^{0}}{0!} + \\frac{0.4^{1}}{1!} + \\frac{0.4^{2}}{2!} \\bigg) \\\\ &= 1 - 1.48 e^{-0.4}. \\end{aligned} \\] q.06 F. Mean of a linear function of rvs (HB p.9) For a random variable \\(Y=-2-2X\\) , then \\[ E(Y) = aE(X) + b, \\] where \\(E(X) = 10\\) , \\(a = -2\\) , and \\(b=-2\\) . Therefore, \\[ E(Y) = -2 (10) + (-2) = -20 - 2 = -22. \\] q.07 E. Variance of a linear function of rvs (HB p.9) For a random variable \\(Y=-2-2X\\) , then \\[ S(Y) = \\sqrt{V(Y)} = \\sqrt{a^{2} V(X)} = |a| \\sqrt{V(X)}, \\] where \\(V(X) = 9\\) and \\(a = -2\\) . Therefore \\[ S(Y) = |-2| \\sqrt{9} = 6. \\] q.08 B. Poisson approximation of rare events (HB p.10) It is estimated that \\(p=1/420\\) suffer from the disease and \\(n=2100\\) employees work for the company. Therefore, as \\(n \\geq 50\\) and \\(p \\leq 0.05\\) , the approximate number of employees who suffer from the disease at the company will be distributed \\[ B \\bigg( 2100, \\frac{1}{420} \\bigg) \\approx \\text{Poisson} (np) \\sim \\text{Poisson} (5) \\] q.09 C. Poisson process (HB p.10) and Poisson distribution (HB p.8) Let \\(X\\) model the number of email arriving in the office per hour. Then \\(X \\sim \\text{Poisson}(\\mu)\\) , where \\(\\mu = 3\\) is the average number of emails received in an hour. The probability \\(P(X=4)\\) is given by \\[ \\begin{aligned} p(4) = e^{-3} \\bigg( \\frac{3^{4}}{4!} \\bigg) \\simeq 0.168. \\end{aligned} \\] q.10 B. Poisson process (HB p.10) and Exponential distribution (HB p.10) Let \\(T\\) model the waiting time between emails arriving in the office. Then \\(T \\sim M(\\lambda)\\) , where \\(\\lambda = 3\\) is the average number of emails received in an hour. Fifteen minutes is \\(1/4\\) hours, so \\(P(T < 1/4)\\) is given by \\[ \\begin{aligned} P \\bigg(T \\leq \\frac{1}{4} \\bigg) = F\\bigg(\\frac{1}{4}\\bigg) = 1 - e^{-3 \\big( \\frac{1}{4} \\big)} \\simeq 0.528. \\end{aligned} \\] q.11 F. Poisson process (HB p.10) The number of emails that will arrive in 6 hours is distributed \\(\\text{Poisson}(\\lambda t)\\) , where \\(\\lambda = 3, t=6\\) . So \\(\\text{Poisson}(18)\\) . q.12 Bernoulli process (HB p.10) and Exponential distribution (HB p.10) B. False , in a Bernoulli process, events occur in discrete trials. F. False , if \\(X\\) has an exponential distribution then \\(X \\sim M(\\lambda)\\) , and so \\(E(X) = 1/\\lambda\\) . q.13 C. Population quantiles rv (HB p.11) : continuous The \\(\\alpha\\) -quantile of a continuous rv \\(X\\) with c.d.f. \\(F(X)\\) is defined as \\(F(x) = \\alpha\\) . If \\(F(x) = 2 - 10/x\\) and \\(\\alpha = 1/2\\) , then \\[ \\begin{aligned} F(x) = \\alpha \\to 2 - \\frac{10}{x} &= \\frac{1}{2} \\\\ 2 - \\frac{1}{2} &= \\frac{10}{x} \\\\ \\frac{3}{2} &= \\frac{10}{x} \\\\ 3x &= 20 \\\\ x &= \\frac{20}{3}. \\end{aligned} \\] q.14 E. Population quantiles of a discrete rv (HB p.11) The smallest value of \\(x \\in X\\) to satisfy \\(F(X) = q_{U} = 3/4\\) is \\(x=5\\) . q.15 A. Difference between two independent normal rvs (HB p.11) If \\(X \\sim N(4,3)\\) , \\(Y \\sim N(2,2)\\) , then \\(U = X - Y\\) will also be distributed normally with parameters: \\[ E(U) = E(X-Y) = E(X) - E(Y) = 4 - 2 = 2, \\] and \\[ V(U) = V(X-Y) = V(X) + V(Y) = 3 + 2 = 5. \\] So \\(U = X - Y \\sim N(2, 5)\\) . q.16 D. Probabilities for a normal distribution (HB p.12) If \\(X \\sim N(22, 36)\\) , then the probability \\(P(X > 25)\\) is given by \\[ \\begin{aligned} P(X > 25) &= 1 - P\\bigg(Z < \\frac{25-22}{\\sqrt{36}} \\bigg) \\\\ &= 1 - P\\bigg(Z < \\frac{1}{2} \\bigg) \\\\ &= 1 - \\Phi(0.50) \\\\ &\\simeq 1 - 0.6915 = 0.3085. \\end{aligned} \\] q.17 F. Quantiles of a normal distribution (HB p.12) The volume of the container (in ml) that ensures overflow will only occur 1% of the time corresponds to the 0.99-quantile of the distribution of \\(X \\sim N(1748.48, 20^{2})\\) , such that \\[ x = \\sigma q_{0.99} + \\mu, \\] where \\(q_{0.99} = 2.326\\) is the 0.99-quantile of the standard normal distribution. Therefore, \\[ x = 20 (2.326) + 1748.48 = 1795.00. \\] q.18 A. Distribution of the mean (HB p.12) and Probabilities for a normal distribution (HB p.12) If \\(X \\sim N(10000, 500^{2})\\) , then \\(\\overline{X}_{50}\\) will be distributed \\[ E(\\overline{X}_{50}) = \\mu = 10000, \\] and \\[ V(\\overline{X}_{50}) = \\frac{\\sigma^{2}}{n} = \\frac{500^{2}}{50} = 5000. \\] Therefore the probability \\(P(\\overline{X}_{50} < 9900)\\) will be given by \\[ \\begin{aligned} P(X < 9900) &= 1 - P \\bigg( Z < \\frac{9900-10000}{\\sqrt{5000}} \\bigg) \\\\ &= P\\bigg(Z < -\\sqrt{2} \\bigg) \\\\ &\\simeq \\Phi(-1.41) \\\\ &= 1 - \\Phi(1.41) \\\\ &= 1 - 0.9207 = 0.0793. \\end{aligned} \\] q.19 A. Mean of an unbiased estimator (HB p.13) If \\(E(X_{1}) = \\theta\\) , \\(E(X_{2}) = 2\\theta\\) , and \\(E(X_{3}) = 6\\theta\\) , then an unbiased estimator for \\(\\theta\\) is such that \\(E(\\widehat \\theta) = \\theta\\) . Looking at option A, \\[ \\begin{aligned} E(\\widehat \\theta) &= E\\bigg\\{ \\frac{1}{10}(2 X_{1} + X_{2} + X_{3}) \\bigg\\} \\\\ &= \\frac{1}{10} E\\bigg\\{ (2 X_{1} + X_{2} + X_{3}) \\bigg\\} \\\\ &= \\frac{1}{10} \\{ E (2 X_{1}) + E (X_{2}) + E (X_{3}) \\} \\\\ &= \\frac{1}{10} \\{ 2 E (X_{1}) + E (X_{2}) + E (X_{3}) \\} \\\\ &= \\frac{1}{10} \\{ 2 \\theta + 2 \\theta + 6 \\theta \\} \\\\ &= \\frac{1}{10} \\{ 10 \\theta \\} \\\\ &= \\theta. \\end{aligned} \\] q.20 C. Exact CI for a normal mean (HB p.15) The upper limit of an exact 99% confidence interval for the mean \\(\\mu^{+}\\) is given by \\[ \\mu^{+} = \\overline{x} + t \\frac{s}{\\sqrt{n}}, \\] where \\(t\\) is the \\((1-(\\alpha/2))\\) -quantile of \\(t(n-1)\\) . Here we have \\(\\overline{x}=112.5\\) , \\(n=10\\) , and \\(s=35.1\\) . The value \\(t=3.250\\) is the \\(0.995\\) -quantile of the \\(t(9)\\) distribution. So the upper limit is given by \\[ \\begin{aligned} \\mu^{+} = 112.5 + 3.250 \\bigg( \\frac{35.1}{\\sqrt{10}} \\bigg) \\simeq 148.6 \\end{aligned} \\] q.21 C and F. Anatomy of a hypothesis test (HB p.17) and \\(z\\) -test (HB p.17) C. True, as the significance level decreases, the rejection region increases. F. True q.22 B. Calculating the test statistic for a \\(z\\) -test (HB p.17) The test statistic is \\[ Z = \\frac{\\overline{X} - \\mu_{0}}{S/\\sqrt{n}} = \\frac{434.2 - 433}{1.1/\\sqrt{36}} \\simeq 6.55. \\] q.23 D. Calculating the power of a test (HB p.18) The power of this two-sided test with a 5% significance level will be \\[ 1 - \\Phi \\bigg( q_{1-(\\alpha/2)} - \\frac{d}{\\sigma/\\sqrt{n}} \\bigg) \\] where \\(q_{1-(\\alpha/2)} = q_{0.975} = 1.960\\) , \\(d=0.6\\) , \\(\\sigma=4.2\\) , and \\(n=196\\) . Therefore, \\[ \\begin{aligned} 1 - \\Phi \\bigg( 1.960 - \\frac{0.6}{4.2/\\sqrt{196}} \\bigg) &\\simeq 1 - \\Phi (-0.4) \\\\ &= 1 - (1 - \\Phi (0.4) \\\\ &= \\Phi (0.4) \\\\ &= 0.5160. \\end{aligned} \\] q.24 C. Chi-squared goodness-of-fit test (HB p.21) The degrees of freedom of a chi-squared goodness-of-fit test is \\(k-p-1\\) , where \\(k=4\\) is the number of categories, and \\(p=0\\) is the number of estimated parameters. Therefore the degrees of freedom is \\(4-0-1=3\\) . q.25 F. Chi-squared goodness-of-fit test (HB p.21) The chi-squared test statistic is \\[ \\chi^{2} = \\sum\\frac{(O_{i} - E{i})^{2}}{E_i}, \\] so \\[ \\chi^{2} = \\frac{4.1^{2}}{106.9} + \\dots + \\frac{(-3.9)^{2}}{11.9} \\simeq 1.53. \\] q.26 B. Least squares estimate (HB 22) : intercept The least squares estimate of \\(\\beta\\) is \\[ \\beta = \\frac{S_{xy}}{S_{xx}} = \\frac{962.35}{457.44} = 2.10377 \\ldots \\] Therefore, the least squares estimate of the intercept, \\(\\alpha\\) , is \\[ \\alpha = 31.96 \u2212 2.10377 (12.34) \\simeq 6.00. \\] q.27 A and E. Hypothesis test of \\(\\widehat{\\beta}\\) (HB p.22) q.28 A and F. Transformations (HB p.23) : skew data The data is left skew, so we go up the ladder of powers. q.29 A. Using a fitted model (HB p.21) The fitted model is \\(\\log{y} = 0.423 + 0.0325 x\\) , so when \\(x=30\\) , \\[ y = e^{0.423 + 0.0325(30)} = e^{1.398} = 4.047. \\] Section B q.30 Finding the c.d.f. of a rv : continuous If \\(f(x) = \\frac{1}{78} (6x^{2} + 2x + 5)\\) , where \\(x \\in (0,3)\\) , then the \\(F(x)\\) will be, \\[ \\begin{aligned} F(x) = \\int_{0}^{x} \\frac{1}{78} (6y^{2} + 2x + 5) \\> dy &= \\frac{1}{78} \\bigg[ \\frac{6}{3} y^{3} + \\frac{2}{2} y^{2} + 5y \\bigg]_{0}^{x} \\\\ &= \\frac{1}{78} \\bigg[ 2 y^{3} + y^{2} + 5y \\bigg]_{0}^{x} \\\\ &= \\frac{1}{78} ( 2 x^{3} + x^{2} + 5x - (0 + 0 + 0)) \\\\ &= \\frac{1}{78} (2 x^{3} + x^{2} + 5x). \\end{aligned} \\] q.31 Calculating probabilities using the c.d.f. (HB p.7) : continuous If \\(F(x) = \\frac{1}{80} (x^{2} + x^{3})\\) , then \\(P(1 \\leq X \\leq 2) = F(2) - F(1)\\) is given by \\[ \\begin{aligned} F(2) - F(1) &= \\frac{1}{80} (2^{2} + 2^{3}) - \\frac{1}{80} (1^{2} + 1^{3}) \\\\ &= \\frac{1}{80} (10) \\\\ &= \\frac{1}{8}. \\end{aligned} \\] q.32 (a) Geometric distribution (HB p.8) Let \\(X\\) be a discrete rv that represents the number of people who need to pass through the metal detector until the next to activate it. Then \\(X\\) is modelled by the geometric distribution, \\(X \\sim G(p)\\) , where \\(p=0.2\\) is the probability that an individual activates the metal detector, so \\(X \\sim G(0.4)\\) . The probability \\(P(X=4)\\) is given by \\[ \\begin{aligned} P(X=4) = (1-p)^{y-1} p = 0.8^{3} 0.2 = 0.1024. \\end{aligned} \\] (b) Binomial distribution (HB p.8) Let \\(Y\\) be a discrete rv that represents the number of people that passes through the metal detector. Then \\(Y\\) is modelled by the binomial distribution, \\(Y \\sim B(n,p)\\) , where \\(n=10\\) is the sample size and \\(p=0.2\\) is the probability that an individual sets off the metal detector, so \\(Y \\sim B(10,0.2)\\) . The probability \\(P(Y=3)\\) is given by \\[ \\begin{aligned} P(Y=3) = \\binom{n}{y} p^y (1-p)^{n-y} = \\binom{10}{3} 0.2^{3} (0.8)^{7} \\simeq 0.2013. \\end{aligned} \\] q.33 (a) Mean of a rv (HB p.9) : continuous The expected value \\(E(X)\\) of a continuous random variable \\(X\\) is defined as \\[ E(X) = \\int_{L}^{U} x \\> f(x) \\> dx, \\hspace{3mm} x \\in (L, U). \\] Therefore, \\[ \\begin{aligned} E(X) = \\int_{0}^{1} x \\frac{3}{2} (1 - x^{2}) &= \\frac{3}{2} \\int_{0}^{1} x - x^{3} \\> dx \\\\ &= \\frac{3}{2} \\bigg[ \\frac{1}{2} x^{2} - \\frac{1}{4} x^{4} \\bigg]_{0}^{1} \\\\ &= \\frac{3}{2} \\bigg\\{ \\frac{1}{2} (1)^{2} - \\frac{1}{4} (1)^{4} - \\bigg( \\frac{1}{2} (0)^{2} - \\frac{1}{4} (0)^{4} \\bigg) \\bigg\\} \\\\ &= \\frac{3}{2} \\bigg\\{ \\frac{1}{4} \\bigg\\} \\\\ &= \\frac{3}{8}. \\\\ \\end{aligned} \\] Hence, \\(E(X)=3/8\\) . (b) Variance of a rv (HB p.9) : continuous The \\(E(X^{2})\\) of a continuous random variable \\(X\\) is defined as \\[ E(X^{2}) = \\int_{L}^{U} x^{2} \\> f(x) \\> dx, \\hspace{3mm} x \\in (L, U). \\] Therefore, \\[ \\begin{aligned} E(X^{2}) = \\int_{0}^{1} x^{2} \\frac{3}{2} (1 - x^{2}) &= \\frac{3}{2} \\int_{0}^{1} x^{2} - x^{4} \\> dx \\\\ &= \\frac{3}{2} \\bigg[ \\frac{1}{3} x^{3} - \\frac{1}{5} x^{5} \\bigg]_{0}^{1} \\\\ &= \\frac{3}{2} \\bigg\\{ \\frac{1}{3} (1)^{3}- \\frac{1}{5} (1)^{5} - \\bigg( \\frac{1}{3} (0)^{3} - \\frac{1}{5} (0)^{5} \\bigg) \\bigg\\} \\\\ &= \\frac{3}{2} \\bigg( \\frac{1}{15} \\bigg) \\\\ &= \\frac{1}{5}. \\\\ \\end{aligned} \\] Hence, \\(E(X^{2})=1/5\\) . (c) Variance of a rv (HB p.9) : continuous The variance \\(V(X)\\) of a continuous random variable \\(X\\) is defined as \\[ V(X) = E\\{ (X-\\mu)^{2} \\} = E(X^{2}) - E(X)^{2} \\] We know that \\(E(X) = 3/5\\) , \\(E(X^{2})=1/5\\) , so \\[ V(X) = \\frac{1}{5} - \\bigg( \\frac{3}{8} \\bigg)^{2} = \\frac{1}{5} - \\frac{9}{64} = \\frac{19}{320} \\simeq 0.0594. \\] Therefore, \\(V(X)=19/320 \\simeq 0.0594\\) , rounded to 3sf. q.34 (a) Finding the likelihood function of a sample (HB p.13) : discrete Using the p.m.f. of a geometric distribution, if \\(p(x_{i}; \\theta) = \\theta(1-\\theta)^{x_{i}-1}\\) , then the likelihood of \\(\\theta\\) , \\(L(\\theta)\\) , will be \\[ \\begin{aligned} L(\\theta) = \\prod_{i=1}^{4} p(x_{i}; \\theta) &= \\theta(1-\\theta)^{x_{1}-1} \\times \\theta(1-\\theta)^{x_{2}-1} \\times \\theta(1-\\theta)^{x_{3}-1} \\times \\theta(1-\\theta)^{x_{4}-1} \\\\ &= \\theta(1-\\theta)^{5-1} \\times \\theta(1-\\theta)^{2-1} \\times \\theta(1-\\theta)^{5-1} \\times \\theta(1-\\theta)^{1-1} \\\\ &= \\theta \\times \\theta \\times \\theta \\times \\theta \\times (1-\\theta)^{4} \\times (1-\\theta)^{1} \\times (1-\\theta)^{4} \\times (1-\\theta)^{0} \\\\ &= \\theta^{4} \\times (1-\\theta)^{4 + 1 + 4 + 0} \\\\ &= (1-\\theta)^{9} \\theta^{4}. \\\\ \\end{aligned} \\] (b) Finding the MLE of an estimator (HB p.13) If \\(L(\\theta) = (1-\\theta)^{9} \\theta^{4}\\) then, by the product rule , \\(L'(\\theta)\\) will be given by \\[ \\begin{aligned} L'(\\theta) &= (1-\\theta)^{9} \\times 4 \\theta^{3} + (-1) (9) (1-\\theta)^{8} \\theta^{4} \\\\ &= 4 \\theta^{3} (1-\\theta)^{9} - 9 \\theta^{4} (1-\\theta)^{8} \\\\ &= \\theta^{3} (1-\\theta)^{8} ( 4 (1-\\theta) - 9 \\theta ) \\\\ &= \\theta^{3} (1-\\theta)^{8} ( 4 - 4 \\theta - 9 \\theta ) \\\\ &= (1-\\theta)^{8} \\theta^{3} ( 4 - 13 \\theta ) \\\\ \\end{aligned} \\] This matches the expression given in the question. (c) Finding the MLE of an estimator (HB p.13) The MLE of \\(\\theta\\) , \\(\\widehat \\theta\\) , is the solution to \\[ L'(\\theta) = (1-\\theta)^{8} \\theta^{3} ( 4 - 13 \\theta ) = 0. \\] Given the range of \\(\\theta > 0\\) , then \\(\\theta^{3}(1-\\theta)^{8} > 0\\) , so this reduces to solving \\[ \\begin{aligned} 0 &= 4 - 13 \\theta \\\\ 13 \\> \\theta &= 4 \\\\ \\theta &= \\frac{4}{13} \\simeq 0.3077. \\end{aligned} \\] So \\(\\widehat \\theta \\simeq 0.3077\\) . q.35 (a) CI for the difference between two proportions (HB p.15) Let \\(n_{1} = 100, x_{1} = 33\\) represent the sample from 2010, and \\(n_{2} = 150, x_{2} = 54\\) represent the sample from 2013. Then the point estimate of the difference \\(\\widehat d\\) between the two samples is \\[ \\widehat d = p_{1} - p_{2} = \\frac{33}{100} - \\frac{54}{150} = 0.33 - 0.36 = -0.03. \\] Therefore, an approximate 95% confidence interval for the population difference, \\((d^{-}, d^{+})\\) , is given by \\[ \\begin{aligned} d^{-} &= -0.03 - 1.96 \\sqrt{ \\frac{0.33(0.67)}{100} + \\frac{0.36(0.64)}{150} } \\simeq -0.15, \\\\ d^{+} &= -0.03 + 1.96 \\sqrt{ \\frac{0.33(0.67)}{100} + \\frac{0.36(0.64)}{150} } \\simeq 0.09. \\end{aligned} \\] Therefore, a realised approximate 95% confidence interval for this scenario would be \\((-0.15, 0.09)\\) . (b) Repeated experiments interpretation of confidence intervals (HB p.14) If this entire trial was repeated independently a large number of times, and on each occasion a 95% confidence interval for the difference between proportions were calculated, then about 95% of these intervals would contain the population value for the difference between the proportion of individuals that thought cannabis should be legalised in 2010 and the proportion of individuals that thought cannabis should be legalised in 2013. (c) Interpreting confidence intervals (M248 Unit 8, activity 14) The approximate 95% confidence interval found was \\((-0.15, 0.09)\\) . Given this interval contains \\(0\\) , it suggests that there is no difference in the proportion of individuals that thought cannabis should be legalised between 2010 and 2013. q.36 (a) The Wilcoxon signed rank test (HB p.19) Let \\(m\\) denote the underlying population median difference in aluminium concentrations between August and November. This is a two-sided test so the hypotheses are \\[ H_{0} : m = 0, \\> H_{1} : m \\neq 0 \\] (b) The Wilcoxon signed rank test (HB p.19) Tree 1 2 3 4 5 6 7 8 9 10 11 12 13 Difference 12.0 12.3 3.1 7.2 5.3 1.0 6.3 0.0 \u22122.2 23.4 2.0 \u22121.2 \u22125.6 Sign of diff + + + + + + + - + + - - Abs value of diff 12.0 12.3 3.1 7.2 5.3 1.0 6.3 0.0 2.2 23.4 2.0 1.2 5.6 Rank of abs value of diff 10 11 5 9 6 1 8 4 12 3 2 7 The value of the test statistic is the sum of the ranks of the positivie differences, so \\[ w_{+} = 10 + 11 + 5 + 9 + 6 + 1 + 8 + 12 + 3 = 65. \\] (c) Normal approximation of the Wilcoxon test statistic (HB p.20) Under the null hypothesis, for a sample of size \\(12\\) (excluding the zero difference), the Wilcoxon signed rank test statistic \\(W_{+}\\) has mean, \\[ \\begin{aligned} E(W_{+}) = \\frac{n(n+1)}{4} = \\frac{12(13)}{4} = 39, \\end{aligned} \\] and variance, \\[ \\begin{aligned} V(W_{+}) = \\frac{n(n+1)(2n+1)}{24} = \\frac{12(13)(25)}{24} = 162.5. \\end{aligned} \\] (d) Normal approximation of the Wilcoxon test statistic (HB p.20) and Interpreting \\(p\\) -values (HB p.18) In this scenario, the observed sum of ranks for the positive differences is \\(w_{+} = 65\\) , so the corresponding observed value of \\(Z\\) is \\[ Z = \\frac{w_{+} - E(W_{+})}{\\sqrt{V(W_{+})}} = \\frac{65-39}{\\sqrt{162.5}} = 2.0396 \\ldots \\simeq 2.04. \\] Using the table of probabilities of the standard normal distribution in the handbook gives \\[ P(Z \\geq 2.04) = 1 - P(Z \\leq 2.04) = 1 - 0.9793 = 0.0207. \\] So, according to the approximation, the probability of obtaining a Wilcoxon signed rank test statistic of \\(65\\) or greater is approximately \\(0.0207\\) . For a two-sided test, the \\(p\\) -value is double this, so \\[ P(Z \\leq -2.04) + P(Z \\geq 2.04) = 2 P(Z \\geq 2.04) = 0.0414. \\] Given that \\(p=0.0414\\) , there is moderate against the null hypothesis that there is no difference in aluminium concentrations in the wood of trees between August and November. The test statistic is greater than zero, so it suggests that the population median difference in aluminium concentrations between August and November is greater than zero, suggesting that there is an increase in Aluminium concentrations. q.37 (a) Interpreting the regression coefficients in multiple linear regression If the value of the percentage of individuals who earned $3500 or more for each occupation increased ( \\(x_{1}\\) ) by one percentage point and the value of the percentage of individuals who were high school graduates ( \\(x_{2}\\) ) remains fixed, then the measure of prestige for each occupation would be expected to rise by 0.599 (b) Using a fitted model (HB p.22) to find the residual value Using the fitted model, a small company with \\(x_{1} = 8\\) and \\(x_{2} = 64\\) is predicted to have annual profits (in \u00a3100,000s) \\[y = 0.930 - 0.3662 (8) + 0.03543 (64) \\simeq 0.268.\\] Given the actual value was \\(y=0.292\\) , this gives a residual value of \\[ w = 0.292 - 0.268 = 0.024. \\] q.38 ==Not answered==","title":"M248 June 2019"},{"location":"Past%20Paper%20Solutions/m248-201906/#section-a","text":"","title":"Section A"},{"location":"Past%20Paper%20Solutions/m248-201906/#section-b","text":"","title":"Section B"}]}